{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Marvin \ud83e\udd16\ud83c\udfd6\ufe0f","text":"<p>Meet Marvin: a batteries-included library for building AI-powered software. Marvin's job is to integrate AI directly into your codebase by making it look and feel like any other function. </p> <p>Marvin introduces a new concept called AI Functions. These functions differ from conventional ones in that they don\u2019t rely on source code, but instead generate their outputs on-demand with AI by using an LLM as a runtime. With AI functions, you don't have to write complex code for tasks like extracting entities from web pages, scoring sentiment, or categorizing items in your database. Just describe your needs, call the function, and you're done!</p> <p>AI functions work with native data types, so you can seamlessly integrate them into any codebase and chain them into sophisticated pipelines. Technically speaking, Marvin transforms the signature of using AI from <code>(str) -&gt; str</code> to <code>(**kwargs) -&gt; Any</code>. We call this \"functional prompt engineering.\"</p> <p>In addition to AI functions, Marvin also introduces more flexible bots. Bots are highly capable AI assistants that can be given specific instructions and personalities or roles. They can use custom plugins and leverage external knowledge, and automatically create a history of every thread. Under the hood, AI functions are actually a type of bot. </p> <p>To make it easy to work with bots, Marvin includes a fully-functional TUI. The TUI tracks threads across multiple bots and even lets you manage your bots through a conversational interface.</p> <p>Developers can use\u00a0Marvin to add AI capabilities wherever they will be most impactful, without needing to start from scratch. Marvin's code is available on GitHub, and say hello on our Discord server!</p> <p>Marvin is built with \ud83d\udc99 by Prefect.</p>"},{"location":"#features","title":"Features","text":"<p>\ud83e\ude84 Write AI functions to process structured data without source code</p> <p>\ud83e\udd16 Build bots that have personalities and follow instructions</p> <p>\ud83d\udda5\ufe0f Chat with bots in a fully-featured TUI</p> <p>\ud83d\udd0c Give your bots new abilities with plugins </p> <p>\ud83d\udcda Store knowledge that bots can access and use</p> <p>\ud83d\udce1 Available as a Python API, interactive CLI, or FastAPI server</p>"},{"location":"#quick-start","title":"Quick start","text":"<ol> <li>Install: <code>pip install marvin</code></li> <li>Chat: <code>marvin chat</code></li> </ol>"},{"location":"#slightly-less-quick-start","title":"Slightly less quick start","text":"<p>Create a bot: <pre><code>marvin bots create ObiWanKenoBot -p \"knows every Star Wars meme\"\n</code></pre> Chat with it: <pre><code>marvin chat -b ObiWanKenoBot\n</code></pre> </p> <p>See the getting started docs for more.</p>"},{"location":"#open-source","title":"Open source","text":"<p>Marvin is open-source with an Apache 2.0 license and built on standards like Pydantic, FastAPI, Langchain, and Prefect. The code is available on GitHub.</p> <p>Construction zone</p> <p>Marvin is under active development and is likely to change. </p>"},{"location":"#coming-soon","title":"Coming soon","text":"<p>\u267b\ufe0f Interactive AI functions</p> <p>\ud83d\uddbc\ufe0f Admin and chat UIs</p> <p>\ud83c\udfd7\ufe0f Advanced data loading and preprocessing</p> <p>\ud83d\udd2d AI observability platform</p> <p>\ud83d\udda5\ufe0f Deployment guides</p> <p>\ud83c\udf81 Quickstarts for common use cases</p>"},{"location":"#when-should-you-use-marvin","title":"When should you use Marvin?","text":"<p>Marvin is an opinionated, high-level library with the goal of integrating AI tools into software development. There are a few major reasons to use Marvin:</p> <ol> <li> <p>You want an AI function that can process structured data. Marvin brings the power of AI to native data structures, letting you build functions that would otheriwse be difficult or even impossible to write. For example, you can use AI functions to make a list of all the animals in a paragraph, generate JSON documents from HTML content, extract keywords that match some criteria, or categorize sentiment -- without any traditional source code.</p> </li> <li> <p>You want an AI assistant in your code. Marvin's bots can follow instructions and hold conversations to solve complex problems. They can use custom plugins and take advantage of external knowledge. They are designed to be integrated into your codebase, but of course you can expose them directly to your users as well!</p> </li> <li> <p>You want to deploy cutting-edge AI technology with confidence, but without having to make too many decisions. Using LLMs successfully requires very careful consideration of prompts, data preprocessing, and infrastructure. Our target user is more interested in using AI systems than building AI systems. Therefore, Marvin is designed to make adopting this technology as straightforward as possible by optimizing for useful outcomes. Marvin's prompts have been hardened by months of real-world use and will continue to improve over time.</p> </li> </ol>"},{"location":"#when-should-you-not-use-marvin","title":"When should you NOT use Marvin?","text":"<p>There are a few reasons NOT to use Marvin:</p> <ol> <li> <p>You want full control of an AI. Marvin is a high-level library and (with few exceptions) does not generally expose LLM configuration to users. We have chosen settings that give the best results under most circumstances, taking Marvin's built-in prompts into consideration.</p> </li> <li> <p>You want an AI copilot for writing code. Marvin's job isn't to help you write source code; it's to help you do things that are difficult or impossible to express in source code. That could range from mundane activities to writing a function that can extract the names of animals commonly found in North America from an email (yes, it's a ridiculous example - but it's possible). Modern LLMs excel at complex reasoning, and Marvin lets you bring that into your code in a way that feels native and natural.</p> </li> <li> <p>You want to use other LLM models. Marvin is designed to run against OpenAI's GPT-4 and GPT-3.5 models. While we may expand those models in the future, we've discovered that prompts designed for one model rarely translate well to others without modification. In order to maximize the usefulness of the library, we've decided to focus on just these popular models for now.</p> </li> <li> <p>You want full control of your prompts. As a \"functional prompt engineering\" platform, Marvin takes user inputs and generates prompts that are likely to deliver the outcome the user wants, even if they are not verbatim what the user said. Marvin does not expect users to send completely raw prompts to the LLM. </p> </li> <li> <p>You're searching for the Ultimate Question. While Marvin is highly intelligent, even he couldn't come up with the Ultimate Question of Life, the Universe, and Everything. If you're seeking existential enlightenment, you might need to look beyond our beloved paranoid android.</p> </li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#how-do-i-report-a-problem","title":"How do I report a problem?","text":"<p>Marvin is under rapid development and has a few sharp edges! If you run into trouble, please open an issue here.</p>"},{"location":"faq/#should-i-use-marvin-or-chatgpt","title":"Should I use Marvin or ChatGPT?","text":"<p>Use both! In a true sense, Marvin is ChatGPT, and anything your OpenAI account has access to (including GPT-4, plugins, and more) is automatically available to Marvin as well. More specifically, Marvin is a client for ChatGPT, not an alternative to it.</p> <p>ChatGPT is a powerful service primarily accessed through its own UI or by making raw API calls. Marvin provides a new way of accessing ChatGPT through a convenient library. It brings ChatGPT into your normal engineering workflow by letting you intermix Python code with AI-powered constructs that are optimized for your use case.</p>"},{"location":"faq/#should-i-use-gpt-4-or-gpt-35","title":"Should I use GPT-4 or GPT-3.5?","text":"<p>Marvin supports multiple LLM models. At this time, models include OpenAI's GPT-4 (<code>gpt-4</code>) and GPT-3.5 (<code>gpt-3.5-turbo</code>). To set the model, use the environment variable <code>MARVIN_OPENAI_MODEL_NAME</code>. Because not every developer has access to GPT-4 (yet), Marvin's default model is GPT-3.5. This guarantees that everyone can use Marvin \"out of the box.\"</p> <p>Performance is much better on GPT-4 than GPT-3.5, though GPT-3.5 is still very good for many use cases. In particular, GPT-4 is better at following instructions over long interactions and staying \"on-script\" throughout an entire conversation. It is much less susceptible to being distracted and can break problems down into manageable pieces more easily. However, it is slower and up to 30x more expensive than GPT-3.5, and is also not yet widely available to all OpenAI accounts. Many of Marvin's prompts were originally written for GPT-3.5, which is one of the reasons the smaller model still has great results. In our experience, prompts optimized for GPT-4 usually fail outright with GPT-3.5.</p>"},{"location":"faq/#python-api","title":"Python API","text":""},{"location":"faq/#how-do-i-run-async-code","title":"How do I run async code?","text":"<p>Marvin is an async library because the vast majority of time is spent waiting for LLM responses to be returned via API. Therefore, it can be used natively in any other async library. </p> <p>The standard Python repl doesn't allow you to directly <code>await</code> async coroutines, but interpreters like IPython do (IPython is included as a Marvin development dependency).</p> <p>To integrate bots into synchronous frameworks, wrap async calls in <code>asyncio.run(coro)</code> or use convenience methods like <code>Bot.say_sync()</code>. Marvin uses a library called <code>nest-asyncio</code> to run nested event loops in a way that Python doesn't usually permit.</p>"},{"location":"faq/#marvin","title":"Marvin","text":""},{"location":"faq/#who-maintains-marvin","title":"Who maintains Marvin?","text":"<p>Marvin is built with \ud83d\udc99 by Prefect.</p>"},{"location":"faq/#is-marvin-open-source","title":"Is Marvin open-source?","text":"<p>Marvin is fully open-source under an Apache 2.0 license.</p>"},{"location":"faq/#where-is-marvins-code","title":"Where is Marvin's code?","text":"<p>Marvin's code can be found on GitHub.</p>"},{"location":"faq/#why-marvin","title":"Why \"Marvin\"?","text":"<pre><code>from marvin import Bot\nbot = Bot()\nresponse = await bot.say(\"Why are you called Marvin?\")\nprint(response.content)\n# Ah, a question of origins! The name \"Marvin\" might be inspired by the\n# character Marvin the Paranoid Android from Douglas Adams' \"The Hitchhiker's\n# Guide to the Galaxy\" series. Marvin the Paranoid Android was an artificially\n# intelligent character with a distinct personality. However, my purpose here is\n# to be a clever, fun, and helpful assistant for you. I hope I can bring a\n# smile to your face while assisting you with your questions and tasks!\n</code></pre>"},{"location":"faq/#what-is-marvin","title":"What... is Marvin?","text":"<p>The first time we released an early version of Marvin in the Prefect Slack, the quality and tone of the answers was so good that some of our team became convinced that the demo was staged, with our CTO operating the Marvin account like a sock puppet. </p> <p>The idea stuck.</p>"},{"location":"faq/#whoa-code","title":"Whoa code","text":"<p>AI functions use LLMs as a runtime and don't need any source code. We call that whoa-code.</p>"},{"location":"development/development/","title":"Development","text":""},{"location":"development/development/#installing-for-development","title":"Installing for development","text":"<p>For development, you should clone the git repo and create an editable install including all development dependencies. To do so, run the following:</p> <pre><code>git clone https://github.com/prefecthq/marvin.git\ncd marvin\npip install -e \".[dev]\"\n</code></pre> <p>Please note that editable installs require <code>pip &gt;= 21.3</code>. To check your version of pip, run <code>pip --version</code> and, if necessary, <code>pip install -U pip</code> to upgrade.</p>"},{"location":"development/development/#static-analysis","title":"Static analysis","text":"<p>In order to merge a PR, code must pass a static analysis check. Marvin uses <code>ruff</code> and <code>black</code> to ensure consistently formatted code. At this time, we do not do a static typing check, but we encourage full type annotation. </p> <p>To run the checks locally, you can use pre-commit.</p> <p>Pre-commit is included as a development dependency. To set it up, run the following from your <code>marvin</code> root directory:</p> <pre><code>pre-commit install\n</code></pre> <p>The pre-commit checks will now be run on every commit you make. You can run them yourself with:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"development/development/#unit-tests","title":"Unit tests","text":"<p>Marvin's unit tests live in the <code>tests/</code> directory. There are two types of unit tests; those that require LLM calls and those that don't. Tests that require LLM calls should be put in the <code>tests/llm_tests</code> directory. They are run separately because they have different time and cost constraints than standard tests.</p> <p>Marvin uses pytest to run tests. To invoke it: <pre><code># run all tests\npytest\n\n# run only LLM tests\npytest -m \"llm\"\n# run only non-LLM tests\npytest -m \"not llm\"\n</code></pre></p>"},{"location":"getting_started/ai_functions_quickstart/","title":"AI Functions","text":"<p>AI functions are functions that are defined locally but use AI to generate their outputs. Like normal functions, AI functions take arguments and return structured outputs like <code>lists</code>, <code>dicts</code> or even Pydantic models. Unlike normal functions, they don't need any source code! </p> <p>Consider the following example, which contains a function that generates a list of fruits. The function is defined with a descriptive name, annotated input and return types, and a docstring -- but doesn't appear to actually do anything. Nonetheless, because of the <code>@ai_fn</code> decorator, it can be called like a normal function and returns a list of fruits.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef list_fruits(n: int) -&gt; list[str]:\n\"\"\"Generate a list of n fruits\"\"\"\nlist_fruits(n=3) # [\"apple\", \"banana\", \"orange\"]\n</code></pre> <p>AI functions are especially useful for activies that would be difficult, time-consuming, or impossible to code. They are particularly powerful for parsing and processing strings, but can be used with almost any data structure. Here are a few more examples:</p> <p><pre><code>@ai_fn\ndef extract_animals(text: str) -&gt; list[str]:\n\"\"\"Returns a list of all animals mentioned in the text\"\"\"\n</code></pre> <pre><code>@ai_fn\ndef classify_sentiment(tweets: list[str]) -&gt; list[bool]:\n\"\"\"\n    Given a list of tweets, classifies each one as \n    positive (true) or negative (false) and returns \n    a corresponding list\n    \"\"\"\n</code></pre> <pre><code>@ai_fn\ndef suggest_title(article: str, style: str=None) -&gt; str:\n\"\"\"\n    Suggest a title for the provided article, optionally in \n    the style of a publication (such as the AP, NYTimes, etc.)\n    \"\"\"\n</code></pre> <pre><code>@ai_fn\ndef extract_keywords(text:str, criteria:str=None) -&gt; list[str]:\n\"\"\"\n    Extract important keywords from text, optionally only including \n    those that meet the provided criteria (for example, \"colors\", \n    \"proper nouns\", or \"European capitals\")\n    \"\"\"\n</code></pre></p> <p>For more information about AI functions, including examples and how to include executable code in your function, see the AI function docs.</p>"},{"location":"getting_started/bots_quickstart/","title":"Bots","text":"<p>Bots are AI assistants that can take instructions over multiple interactions. </p> <p>To create a new interactive bot, instantiate the <code>Bot</code> class with instructions, a personality, or plugins. You can begin talking to it with the <code>say()</code> method. Bots have a memory, so if you call <code>say()</code> again, the bot will recall your conversation.</p> <p>Note</p> <p>Marvin is an async library and the <code>say()</code> method must be awaited. Bots also have a synchronous <code>say_sync()</code> method for convenience.</p> <pre><code>from marvin import Bot\nbot = Bot(personality='knows every Star Wars meme')\nawait bot.say('Hello there')\nawait bot.say('How do you feel about sand?')\n</code></pre> <p>By combining personalities, instructions, and plugins, you can get bots to solve complex problems that would be difficult to address in traditional code. Bots can also be exposed directly to users to act as assistants or interactive guides.</p> <p>For more information about bots, see the bot docs.</p>"},{"location":"getting_started/chat_quickstart/","title":"Chat","text":""},{"location":"getting_started/chat_quickstart/#quick-chat","title":"Quick chat","text":"<p>To quickly jump into a chat, run <code>marvin chat</code> from your command line. This will open a new session with the default chatbot, whose name is Marvin. You can type messages to the bot, and it will respond. </p> <pre><code>marvin chat\n</code></pre> <p>To learn more about the Marvin TUI, see its documentation</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>Marvin requires Python 3.9+.</p>"},{"location":"getting_started/installation/#for-normal-use","title":"For normal use","text":"<p>To install Marvin, run <code>pip install marvin</code>. As a matter of best practice, we recommend installing Marvin in a virtual environment.</p> <p>To use Marvin's knowledge features, please include the Chroma dependency: <code>pip install marvin[chromadb]</code>.</p> <p>Before using Marvin, you'll need to configure your OpenAI API key.</p>"},{"location":"getting_started/installation/#for-development","title":"For development","text":"<p>To install Marvin for development, please see the development guide.</p>"},{"location":"getting_started/openai/","title":"OpenAI","text":"<p>Marvin uses the OpenAI API to access state-of-the-art language models, including GPT-4 and GPT-3.5. These models are capable of generating natural language text that is often indistinguishable from human-written text. By integrating with the OpenAI API, Marvin provides a simple interface for accessing these powerful language models and incorporating them into your own applications.</p> <p>To use Marvin, you need to obtain an OpenAI API key and set it as an environment variable. This API key is used to authenticate your requests to the OpenAI API. This is the only required configuration for using Marvin.</p>"},{"location":"getting_started/openai/#getting-an-api-key","title":"Getting an API key","text":"<p>To obtain an OpenAI API key, follow these steps:</p> <ol> <li>Log in to your an OpenAI account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create new secret key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you will not be able to see it again.</li> </ol>"},{"location":"getting_started/openai/#configuring-your-api-key","title":"Configuring your API key","text":""},{"location":"getting_started/openai/#marvin-cli","title":"Marvin CLI","text":"<p>The easiest way to set your API key is by running <code>marvin setup-openai</code>, which will let you store your API key in your Marvin configuration file. </p>"},{"location":"getting_started/openai/#environment-variables","title":"Environment variables","text":"<p>Alternatively, you can provide your API key as an environment variable (as with any Marvin setting). Marvin will check <code>MARVIN_OPENAI_API_KEY</code> followed by <code>OPENAI_API_KEY</code>. The latter is more standard and may be accessed by multiple libraries, but the former can be used to scope the API key for Marvin's use only. These docs will use <code>MARVIN_OPENAI_API_KEY</code> but either will work.</p> <p>To set your OpenAI API key as an environment variable, open your terminal and run the following command, replacing  with the actual key: <pre><code>export MARVIN_OPENAI_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This will set the key for the duration of your terminal session. To set it more permanently, configure your terminal or its respective env files.</p>"},{"location":"guide/concepts/ai_functions/","title":"\ud83e\ude84 AI Functions","text":"<p>Features</p> <p>\ud83c\udf89 Create AI functions with a single <code>@ai_fn</code> decorator</p> <p>\ud83e\uddf1 Use native data structures (or Pydantic models!) as inputs and outputs</p> <p>\ud83d\udd17 Chain or nest calls to create functional AI pipelines</p> <p>\ud83e\uddd9 Add features to code that would be difficult or impossible to write yourself</p> <p>AI functions are functions that are defined locally but use AI to generate their outputs. Like normal functions, AI functions take arguments and return structured outputs like <code>lists</code>, <code>dicts</code> or even Pydantic models. Unlike normal functions, they don't need any source code! </p> <p>Consider the following example, which contains a function that generates a list of fruits. The function is defined with a descriptive name, annotated input and return types, and a docstring -- but doesn't appear to actually do anything. Nonetheless, because of the <code>@ai_fn</code> decorator, it can be called like a normal function and returns a list of fruits.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef list_fruits(n: int) -&gt; list[str]:\n\"\"\"Generate a list of n fruits\"\"\"\nlist_fruits(n=3) # [\"apple\", \"banana\", \"orange\"]\n</code></pre> <p>Tip</p> <p>AI functions work best with GPT-4, but results are still very good with GPT-3.5.</p>"},{"location":"guide/concepts/ai_functions/#when-to-use-ai-functions","title":"When to use AI functions","text":"<p>Because AI functions look and feel just like normal functions, they are the easiest way to add AI capabilities to your code -- just write the definition of the function you want to call, and use it anywhere! However, though they can feel like magic, it's important to understand that there are times you should prefer not to use AI functions.</p> <p>Modern LLMs are extremely powerful, especially when working with natural language and ideas that are easy to discuss but difficult to describe algorithmically. However, since they don't actually execute code, computing extremely precise results can be surprisingly difficult. Asking an AI to compute an arithmetic expression is a lot like asking a human to do the same -- it's possible they'll get the right answer, but you'll probably want to double check on a calculator. On the other hand, you wouldn't ask the calculator to rewrite a paragraph as a poem, which is a perfectly natural thing to ask an AI. Bear in mind that AI functions are (relatively) slow and expensive compared to running code on your computer. </p> <p>Therefore, while there are many appropriate times to use AI functions, it's important to note that they complement normal functions incredibly well and to know when to use one or the other. AIs tend to excel at exactly the things that are very hard to describe algorithmically. If you're doing matrix multiplication, use a normal function. If you're extracting all the animals that are native to Europe from text, use an AI function.</p> <p>Here is a guide for when to use AI functions:</p> <ul> <li>Generating data (any kind of text, but also data matching a certain structure or template)</li> <li>Translating or rewriting text</li> <li>Summarization</li> <li>Sentiment analysis</li> <li>Keyword or entity extraction</li> <li>Asking qualitative questions about quantitative data</li> <li>Fixing spelling or other errors</li> <li>Generating outlines or action items</li> <li>Transforming one data structure to another</li> </ul> <p>Here is a guide for when NOT to use AI functions:</p> <ul> <li>The function is easy to write normally</li> <li>You want to be able to debug the function</li> <li>You require deterministic outputs</li> <li>Precise math beyond basic arithmetic</li> <li>You need any type of side effect or IO (AI functions are not \"executed\" in a traditional sense, so they can't interact with your computer or network)</li> <li>The objective is TOO magic (tempting though it may be, you can't write an AI function to solve an impossible problem)</li> </ul>"},{"location":"guide/concepts/ai_functions/#basic-usage","title":"Basic usage","text":"<p>The <code>ai_fn</code> decorator can be applied to any function. For best results, the function should have an informative name, annotated input types, a return type, and a docstring. The function does not need to have any source code written, but advanced users can add source code to influence the output in two different ways (see \"writing source code\")</p> <p>When a <code>ai_fn</code>-decorated function is called, all available information is sent to the AI, which generates a predicted output. This output is parsed and returned as the function result.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef my_function(input: Type) -&gt; ReturnType:\n\"\"\" \n    A docstring that describes the function's purpose and behavior.\n    \"\"\"\n# call the function\nmy_function(input=\"my input\")\n</code></pre> <p>Note the following:</p> <ol> <li>Apply the decorator to the function. It does not need to be called (though it can take optional arguments)</li> <li>The function should have a descriptive name</li> <li>The function's inputs should have type annotations</li> <li>The function's return type should be annotated</li> <li>The function has a descriptive docstring</li> <li>The function does not need any source code!</li> </ol>"},{"location":"guide/concepts/ai_functions/#advanced-usage","title":"Advanced usage","text":""},{"location":"guide/concepts/ai_functions/#customizing-the-llm","title":"Customizing the LLM","text":"<p>By default, AI functions use Marvin's global LLM settings. However, you can change this on a per-function basis by providing a valid model name or temperature to the <code>@ai_fn</code> decorator.</p> <p>For example, this function will always use GPT-3.5 with a temperature of 0.2. <pre><code>from marvin import ai_fn\n@ai_fn(llm_model_name='gpt-3.5-turbo', llm_model_temperature=0.2)\ndef my_function():\n...\n</code></pre></p>"},{"location":"guide/concepts/ai_functions/#deterministic-ai-functions","title":"Deterministic AI functions","text":"<p>LLM implementations like ChatGPT are non-deterministic; that is, they do not always return the same output for a given input. In some use cases, like natural conversation, this is desireable. In others, especially programmatic ones, it is not. You can control this by setting the model <code>temperature</code>. High temperature leads to greater variation in responses; a temperature of 0 will always give the same response for a given input. Marvin's default temperature is 0.8. To create a deterministic AI function, set its temperature to 0.</p> <p>Upgrading Marvin may change AI function outputs</p> <p>Marvin wraps your AI function with additional prompts in order to get the LLM to generate parseable outputs. We are constantly adjusting those prompts to improve performance and address edge cases. Therefore, even with a temperature of 0, what your AI function sends to the LLM might change if you upgrade from one version of Marvin to another, resulting in different AI function outputs. Therefore, AI functions with <code>temperature=0</code> are only guaranteed to be deterministic for a specific Marvin version. </p>"},{"location":"guide/concepts/ai_functions/#calling-the-function","title":"Calling the function","text":"<p>By default, the <code>ai_fn</code> decorator will call your function and supply the return value to the AI. For functions without source code, this obviously has no consequence. However, you can take advantage of this fact to influence the AI result by returning helpful or preprocessed outputs. Since the AI sees the source code as well as the return value, you can also influence it through comments. </p> <p>You can see this strategy used in the example that summarizes text from Wikipedia. In the example, the function takes in a page's title and uses it to load the page's content. The content is returned and used by the AI for summarization.</p> <p>To disable this behavior entirely, call the decorator as <code>@ai_fn(call_function=False)</code>.</p>"},{"location":"guide/concepts/ai_functions/#async-functions","title":"Async functions","text":"<p>The <code>ai_fn</code> decorator works with async functions.</p> <pre><code>from marvin import ai_fn\n@ai_fn\nasync def f(x: int) -&gt; int:\n\"\"\"Add 100 to x\"\"\"\nawait f(5)\n</code></pre>"},{"location":"guide/concepts/ai_functions/#complex-annotations","title":"Complex annotations","text":"<p>Annotations don't have to be types; they can be complex objects or even string descriptions. For inputs, the annotation is transmitted to the AI as-is. Return annotations are processed through Marvin's <code>ResponseFormatter</code> mechanism, which puts extra emphasis on compliance. This means you can supply complex instructions in your return annotation. However, note that you must include the word <code>json</code> in order for Marvin to automatically parse the result into native objects!</p> <p>Therefore, consider these two approaches to defining an output: <pre><code>from marvin import ai_fn\n@ai_fn\ndef fn_with_docstring(n: int) -&gt; list[dict]:\n\"\"\"\n    Generate a list of n people with names and ages\n    \"\"\"\n@ai_fn\ndef fn_with_string_annotation(n: int) -&gt; 'a json list of dicts that have keys for name and age':\n\"\"\"\n    Generate a list of n people\n    \"\"\"\nclass Person(pydantic.BaseModel):\nname: str\nage: int\n@ai_fn\ndef fn_with_structured_annotation(n: int) -&gt; list[Person]:\n\"\"\"\n    Generate a list of n people\n    \"\"\"\n</code></pre> All three of these functions will give similar output (though the last one, <code>fn_with_structured_annotation</code>, will return Pydantic models instead of dicts). However, they are increasingly specific in their instructions to the AI. While you should always try to make your intent as clear as possible to the AI, you should also choose an approach that will make sense to other people reading your code. This would lead us to probably prefer the first or third functions over the second, which doesn't look like a typical Python function.</p>"},{"location":"guide/concepts/ai_functions/#plugins","title":"Plugins","text":"<p>AI functions are powered by bots, so they can also use plugins. Unlike bots, AI functions have no plugins available by default in order to minimize the possibility of confusing behavior. You can provide plugins when you create the AI function:</p> <pre><code>@ai_fn(plugins=[...])\ndef my_function():\n...\n</code></pre>"},{"location":"guide/concepts/ai_functions/#examples","title":"Examples","text":""},{"location":"guide/concepts/ai_functions/#generate-a-list-of-fruits","title":"Generate a list of fruits","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef list_fruits(n: int) -&gt; list[str]:\n\"\"\"Generate a list of n fruits\"\"\"\nlist_fruits(3) # [\"apple\", \"banana\", \"orange\"]\n</code></pre>"},{"location":"guide/concepts/ai_functions/#generate-fake-data-according-to-a-schema","title":"Generate fake data according to a schema","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef fake_people(n: int) -&gt; list[dict]:\n\"\"\"\n    Generates n examples of fake data representing people, \n    each with a name and an age.\n    \"\"\"\nfake_people(3)\n# [{'name': 'John Doe', 'age': 29},\n#  {'name': 'Jane Smith', 'age': 34},\n#  {'name': 'Alice Johnson', 'age': 42}]\n</code></pre>"},{"location":"guide/concepts/ai_functions/#correct-spelling-and-grammar","title":"Correct spelling and grammar","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef fix_sentence(sentence: str) -&gt; str:\n\"\"\"\n    Fix all grammatical and spelling errors in a sentence\n    \"\"\"\nfix_sentence(\"he go to mcdonald and buy burg\") # \"He goes to McDonald's and buys a burger.\"\n</code></pre>"},{"location":"guide/concepts/ai_functions/#unit-testing-llms","title":"Unit testing LLMs","text":"<p>One of the difficulties of building an AI library is unit testing it! While it's possible to make LLM outputs deterministic by setting the temperature to zero, a small change to a prompt could result in very different outputs. Therefore, we want a way to assert that an LLM's output is \"approximately equal\" to an expected value.</p> <p>This example is actually used by Marvin itself! See <code>marvin.utilities.tests.assert_llm()</code>.</p> <pre><code>@ai_fn()\ndef assert_llm(output: Any, expectation: Any) -&gt; bool:\n\"\"\"\n    Given the `output` of an LLM and an expectation, determines whether the\n    output satisfies the expectation.\n    For example:\n        `assert_llm(5, \"output == 5\")` will return `True` \n        `assert_llm(5, 4)` will return `False` \n        `assert_llm([\"red\", \"orange\"], \"a list of colors\")` will return `True` \n        `assert_llm([\"red\", \"house\"], \"a list of colors\")` will return `False`\n    \"\"\"\nassert_llm('Hello, how are you?', expectation='Hi there') # True\n</code></pre>"},{"location":"guide/concepts/ai_functions/#summarize-text","title":"Summarize text","text":"<p>This function takes any text and summarizes it. See the next example for a function that can also access Wikipedia automatically.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef summarize(text: str) -&gt; str:\n\"\"\"\n    Summarize the provided text\n    \"\"\"\nimport wikipedia\npage = wikipedia.page('large language model')\nsummarize(text=page.content)\n# Large language models (LLMs) are neural networks with billions of parameters\n# trained on massive amounts of unlabelled text. They excel at various tasks and\n# can capture much of human language's syntax and semantics. LLMs use the\n# transformer architecture and are trained using unsupervised learning. Their\n# applications include fine-tuning and prompting for specific natural language\n# processing tasks.\n</code></pre>"},{"location":"guide/concepts/ai_functions/#summarize-text-after-loading-a-wikipedia-page","title":"Summarize text after loading a Wikipedia page","text":"<p>This example demonstrates how <code>ai_fn</code> can call a function to get additional information that can be used in producing a result. Here, the function downloads content from Wikipedia given a title.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef summarize_from_wikipedia(title: str) -&gt; str:\n\"\"\"\n    Loads the wikipedia page corresponding to the provided \n    title and returns a summary of the content.\n    \"\"\"\nimport wikipedia\npage = wikipedia.page(title)\n# the content to summarize\nreturn page.content\nsummarize_from_wikipedia(title='large language model')\n# A large language model (LLM) is a language model consisting of a neural\n# network with many parameters (typically billions of weights or more), trained\n# on large quantities of unlabelled text using self-supervised learning. LLMs\n# emerged around 2018 and perform well at a wide variety of tasks. This has\n# shifted the focus of natural language processing research away from the\n# previous paradigm of training specialized supervised models for specific\n# tasks.\n</code></pre>"},{"location":"guide/concepts/ai_functions/#suggest-a-title-after-loading-a-url","title":"Suggest a title after loading a URL","text":"<p>This example demonstrates how <code>ai_fn</code> can call a function to get additional information that can be used in producing a result. Here, the function loads an article and then suggests a title for it.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef suggest_title(url: str) -&gt; str:\n\"\"\"\n    Suggests a title for the article found at the provided URL\n    \"\"\"\nimport httpx\n# load the url\nresponse = httpx.get(url)\n# return the url contents \nreturn marvin.utilities.strings.html_to_content(response.content)\nsuggest_title(url=\"https://techcrunch.com/2023/03/14/openai-releases-gpt-4-ai-that-it-claims-is-state-of-the-art/\")\n# OpenAI Releases GPT-4: State-of-the-Art AI Model with Improved Image and Text Understanding\n</code></pre>"},{"location":"guide/concepts/ai_functions/#generate-rhymes","title":"Generate rhymes","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef rhyme(word: str) -&gt; str:\n\"\"\"\n    Generate a word that rhymes with the supplied `word`\n    \"\"\"\nrhyme(\"blue\") # glue\n</code></pre>"},{"location":"guide/concepts/ai_functions/#find-words-meeting-specific-criteria","title":"Find words meeting specific criteria","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef find_words(text: str, criteria: str) -&gt; list[str]:\n\"\"\"\n    Given text and some criteria, returns a list of \n    every word meeting that criteria.\n    \"\"\"\ntext = \"The quick brown fox jumps over the lazy dog.\"\nfind_words(text, criteria=\"adjectives\") # [\"quick\", \"brown\", \"lazy\"]\nfind_words(text, criteria=\"colors\") # [\"brown\"]\nfind_words(text, criteria=\"animals that aren't dogs\") # [\"fox\"]\n</code></pre>"},{"location":"guide/concepts/ai_functions/#suggest-emojis","title":"Suggest emojis","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef get_emoji(text: str) -&gt; str:\n\"\"\"\n    Returns an emoji that describes the provided text.\n    \"\"\"\nget_emoji(\"incredible snack\") # '\ud83c\udf7f'\n</code></pre>"},{"location":"guide/concepts/bots/","title":"\ud83e\udd16 Bots","text":"<p>Features</p> <p>\ud83e\udd16 Create bots with distinct personalities and instructions</p> <p>\ud83d\udd0c Use plugins to give bots new capabilities</p> <p>\ud83d\udcac Persistent memories let you resume threads from different sessions</p> <p>\ud83d\udce1 Talk to bots from Python, your CLI, or the Marvin REST API</p> <p>One of Marvin's central abstractions is the <code>Bot</code> class. A bot is an interface to send text to an AI and receive a response that aligns with the user's objective as much as possible. Marvin allows users to customize this behavior in ways that can transform bots from \"AI assistants\" to reusable programs.</p>"},{"location":"guide/concepts/bots/#when-to-use-bots","title":"When to use bots","text":"<p>Bots are different than AI functions and, in some ways, are more powerful. In fact, AI functions are actually powered by bots. AI functions are designed to take well-scoped problems and turn them into familiar, reusable functions. While bots can be used for the same purpose, they are more appropriate for complex, multi-step interactions or problem solving. </p> <p>AI functions are designed to make the AI invisible. Bots bring the AI to the forefront.</p>"},{"location":"guide/concepts/bots/#python","title":"Python","text":""},{"location":"guide/concepts/bots/#interactive-use","title":"Interactive use","text":"<p>To create a bot, instantiate the bot class.  <pre><code>from marvin import Bot\nford_bot = Bot(\nname=\"Ford\", \npersonality=\"Can't get the hang of Thursdays\", \ninstructions=\"Always responds as if researching an article for the Hitchhiker's Guide to the Galaxy\"\n)\n</code></pre></p> <p>You can immediately talk to the bot by calling its <code>say()</code> method, which is an async coroutine.</p> <pre><code>await ford_bot.say(\"Hello!\")\n</code></pre>"},{"location":"guide/concepts/bots/#history","title":"History","text":"<p>When you speak with a bot, every message is automatically stored. The bot uses its <code>history</code> module to access these messages, which means you can refer to earlier parts of your conversation without any extra work. </p> <p>In Marvin, each conversation is called a <code>thread</code>. Bots generate a new thread each time they are instantiated.  </p> <p>Clear the thread and start a new one by calling <code>Bot.reset_thread()</code>. </p> <p>Resume a specific thread by calling <code>Bot.set_thread()</code>.</p>"},{"location":"guide/concepts/bots/#saving-bots","title":"Saving bots","text":"<p>Bots can be saved to the database by calling the <code>Bot.save()</code> method. Bots are saved by name. </p> <pre><code>bot = Bot(\"Ford\")\nawait bot.save()\n</code></pre> <p>Use the <code>if_exists</code> argument to customize what happens if a bot with the same name already exists:</p> <ul> <li><code>None</code> (the default): an error will be raised.</li> <li><code>\"delete\"</code>: the old bot will be deleted and a new bot will be saved. The new bot will not share anything with the old bot, including its message history.</li> <li><code>\"update\"</code>: the old bot will be updated with the new bot's details. The ID and message history will be preserved.</li> <li><code>\"cancel\"</code>: no changes are made and the old bot is left untouched, but no error is raised.</li> </ul> <pre><code>bot = Bot(\"Ford\")\nawait bot.save(if_exists=\"update\")\n</code></pre>"},{"location":"guide/concepts/bots/#loading-bots","title":"Loading bots","text":"<p>Bots can be loaded with the <code>Bot.load()</code> method.</p> <pre><code>bot = await Bot.load(\"Ford\")\n</code></pre>"},{"location":"guide/concepts/bots/#interactive-chats","title":"Interactive Chats","text":"<p>To quickly jump into an interactive chat with a bot:</p> <pre><code>bot.interactive_chat()\n</code></pre> <p>This will launch the Marvin TUI with the current bot already selected. Note this will also save the bot to the database, overwriting any existing bot with the same name. </p> <p>To launch a simpler experience, pass <code>tui=False</code> when you call the function.</p>"},{"location":"guide/concepts/bots/#streaming-responses","title":"Streaming responses","text":"<p>By default, bots process an entire response and return it as a structured object. However, you can get a streaming response by providing a custom <code>on_token_callback</code> function. Every time the bot generates a token, the callback will be called with a buffer of all the tokens generated to that point. You can access the most recent token as <code>buffer[-1]</code>. </p> <p>For example, to print each token as it's generated: <pre><code>bot = marvin.Bot()\nawait bot.say(\"Hello!\", on_token_callback=lambda buffer: print(buffer[-1]))\n</code></pre></p>"},{"location":"guide/concepts/bots/#async-and-sync-methods","title":"Async and sync methods","text":"<p>Above you've seen how to use asynchronous methods. If needed, synchronous convenience methods are also provided. Just append <code>_sync</code> to the asynchronous method name.</p> <p>Synchronous substitue for async <code>say()</code> method: <pre><code>ford_bot.say_sync(\"Hello again!\")\n</code></pre></p> <p>Synchronous substitute for async <code>save()</code> method: <pre><code>bot = Bot(\"Ford\")\nbot.save_sync()\n</code></pre></p> <p>Synchronous substitute for async <code>load()</code> method: <pre><code>bot = Bot.load_sync(\"Ford\")\n</code></pre></p> <p>Callbacks can be synchronous or asynchronous, and both can be used with the bot's async <code>say()</code> or synchronous <code>say_sync()</code> methods.</p>"},{"location":"guide/concepts/bots/#tui","title":"TUI","text":""},{"location":"guide/concepts/bots/#interactive-use_1","title":"Interactive use","text":"<p>To chat with a bot from your terminal, run <code>marvin chat</code></p> <p></p>"},{"location":"guide/concepts/bots/#loading-an-existing-bot","title":"Loading an existing bot","text":"<p>If you have saved a bot, you can load it in the CLI by using the <code>-b</code> flag and providing the bot's name:</p> <pre><code>marvin chat -b Arthur\n</code></pre>"},{"location":"guide/concepts/bots/#customization","title":"Customization","text":""},{"location":"guide/concepts/bots/#name","title":"Name","text":"<p>Names are unique identifiers that make it easy to reference a specific bot.</p>"},{"location":"guide/concepts/bots/#instructions","title":"Instructions","text":"<p>Instructions define the bot's behavior by specifying how it should respond to questions. For example, the default instructions are to assist the user. However, more utilitarian bots might be instructed to only respond with JSON (or with a specific JSON schema), extract keywords, always rhyme, etc. Bots, especially GPT-4 bots, should not go against their instructions at any time.</p> <p>!!! \"Customizing Instruction Templates\"     While the default instruction template serves most Bot use-cases well, it can be customized by passing a <code>instructions_template: str</code> to a <code>Bot</code>. Note that     the following variables will still be passed to your instructions template when the bot retrieves its instructions: <code>name</code>, <code>instructions</code>, <code>response_format</code>,      <code>personality</code>, <code>date</code>. Warning: Custom instruction templates may cause incompatibilities with future Marvin versions.</p>"},{"location":"guide/concepts/bots/#personality","title":"Personality","text":"<p>Personality affects the style of a bot's responses. For example different bots can have different tones, senses of humor, or frequency of confirmation checks. Personality can include a persona or role as well as interests, fears, or broad objectives. </p> <p>By combining personality and instructions, bot instances can produce complex behavior that can be very different from what users might expect from a chat interface. For example, you could instruct a bot to always respond in a certain way, but use personality to have it act a role (such as a coach, engineer, or therapist).</p>"},{"location":"guide/concepts/bots/#plugins","title":"Plugins","text":"<p>Plugins allow bots to access new information and functionality. By default, bots have plugins that let them browse the internet, visit URLs, and run simple calculations.</p>"},{"location":"guide/concepts/bots/#formatting-responses","title":"Formatting responses","text":"<p>You can optionally enforce certain formats for the bot's responses. In some cases, you can also validate and even parse the resulting output into native objects.</p> <p>Please note: complex response formatting is significantly better with GPT-4 than GPT-3.5.</p> <p>To set up formatting, you need to supply a <code>ResponseFormatter</code> object that defines formatting, validation, and parsing. </p> <p>As a convenience, Marvin also supports a shorthand way of defining formats that will let the library select the most appropriate <code>ResponseFormatter</code> automatically. Shorthand formats can include natural language descriptions, Python types, JSON instructions, or Pydantic models. </p> <p>Here are examples of various shorthand formats:</p>"},{"location":"guide/concepts/bots/#python-types","title":"Python types","text":"<p>Supply Python types to have the bot enforce the appropriate return types. Python types are always validated and parsed.</p> <pre><code>bot = Bot(response_format=bool)\nresponse = await bot.say('Are these statements equivalent? 1: The coffee is hot. 2: The coffee is scalding.')\nprint(response.parsed_content) # True\n</code></pre> <pre><code>bot = Bot(response_format=list[dict[str, int]])\nresponse = await bot.say(\"Format this: (x is 1, y is two), (a is 3, b is 4)\")\nprint(response.parsed_content) # [{'x': 1, 'y': 2}, {'a': 3, 'b': 4}]\n</code></pre>"},{"location":"guide/concepts/bots/#natural-language","title":"Natural language","text":"<p>You can describe the format you want the bot to use, and it will do its best to follow your instructions. If your description includes the word \"json\", then it will also be parsed and validated (you can enable this explicitly by providing a <code>JSONResponseFormatter</code>).</p> <pre><code>bot = Bot(response_format=\"a JSON list of strings\")\nresponse = await bot.say(\"Which of these are capitalized: Apple, banana, cherry, Date, elephant\")\nprint(response.parsed_content) # [\"Apple\", \"Date\"]\n</code></pre> <pre><code>bot = Bot(response_format=\"a hyphenated list\")\nresponse = await bot.say(\"Which of these are capitalized: Apple, banana, cherry, Date, elephant\")\nprint(response.parsed_content) # '- Apple\\n- Date'\n</code></pre> <pre><code>bot = Bot(response_format='&lt;animal&gt; like to eat &lt;food&gt; and live in &lt;biome&gt;')\nresponse = await bot.say('tell me about foxes')\nprint(response.parsed_content) # \"Foxes like to eat small mammals and live in forests.\"\n</code></pre>"},{"location":"guide/concepts/bots/#pydantic","title":"Pydantic","text":"<pre><code>class MyFormat(BaseModel):\nx: int\ny: str = Field(\"The written form of x\")\nbot = Bot(response_format=MyFormat)\nresponse = await bot.say(\"Generate output where x is 22\")\nprint(response.parsed_content) # MyFormat(x=22, y='Twenty-two')\n</code></pre>"},{"location":"guide/concepts/infra/","title":"\u2699\ufe0f Infra","text":"<p>Construction zone</p> <p>This area of the docs is under active development and may change.</p>"},{"location":"guide/concepts/infra/#databases","title":"Databases","text":""},{"location":"guide/concepts/infra/#sqlite","title":"Sqlite","text":"<p>By default, Marvin uses a Sqlite database located at <code>~/.marvin/marvin.sqlite</code>. You can customize this by setting <code>MARVIN_DATABASE_CONNECTION_URL</code> to <code>sqlite+aiosqlite:////{path/to/database}</code>.</p>"},{"location":"guide/concepts/infra/#postgres","title":"Postgres","text":"<p>Marvin can also use Postgres (though this isn't as actively tested at this time). To do so, install the postgres extra: <code>pip install \"marvin[postgres]\"</code> and set <code>MARVIN_DATABASE_CONNECTION_URL</code> to <code>postgresql+asyncpg://{username}:{password}@{hots}:{port}/{database}</code>, filling all variables appropriately.</p>"},{"location":"guide/concepts/infra/#migrations","title":"Migrations","text":"<p>Marvin keeps the database schema up-to-date with Alembic migrations. If Marvin detects an empty database, it will run the initial migration update automatically. However, subsequent migrations will not be run automatically (to avoid any conflicts). Instead, Marvin checks to see if the database is up-to-date on startup and prints a warning if it isn't. You can disable this behavior by setting <code>MARVIN_DATABASE_CHECK_MIGRATION_VERSION_ON_STARTUP=0</code>.</p> <p>After upgrading Marvin, or when you see the warning described above, you should upgrade the database by running:</p> <pre><code>marvin database upgrade\n</code></pre> <p>You will be asked to confirm the upgrade; pass <code>-y</code> to do so automatically (this can be useful in CI). The upgrade command is idempotent and safe to run multiple times; the database is only modified if necessary.</p>"},{"location":"guide/concepts/infra/#chroma","title":"Chroma","text":"<p>Marvin provides a simple wrapper of the ChromaDB client to make it easier to interact with the database.</p> <p>ChromaDB has a large memory footprint and is an optional dependency</p> <p>ChromaDB uses <code>sentence-transformers</code> by default for embeddings, which requires <code>torch</code>. <code>torch</code> has recently added wheels for Python 3.11.</p> <p>Although Marvin uses OpenAI's \"text-embedding-ada-002\" model offered via <code>chromadb.utils.embedding_functions</code>, <code>chromadb</code> enforces the <code>sentence-transformers</code> dependency at this time.</p> <p>Install the <code>chromadb</code> extra with <code>pip install marvin[chromadb]</code> to use ChromaDB.</p> <p>Read the ChromaDB usage guide for more information.</p>"},{"location":"guide/concepts/infra/#relevance-to-marvin","title":"Relevance to Marvin","text":"<p>ChromaDB is an embeddings database that is used by Marvin to store and query document embeddings.</p> <p>When you call <code>.load_and_store()</code> on a <code>Loader</code>, you are calling <code>Chroma.add</code> to store documents in the default collection.</p> <p><code>load_and_store</code> accepts an optional <code>topic_name</code> that corresponds to a collection in ChromaDB. If you want to store documents in a different collection, simply pass a different <code>topic_name</code> to <code>load_and_store</code> and the collection will be created for you or updated.</p>"},{"location":"guide/concepts/infra/#usage","title":"Usage","text":"<p>If desired, you can use it directly:</p>"},{"location":"guide/concepts/infra/#querying","title":"Querying","text":"<pre><code>from marvin.infra.chroma import Chroma\nasync with Chroma() as chroma:\nquery_results: dict[str, list] = await chroma.query(\nquery_texts=[\"some natural language query\"],\nwhere={\"some_metadata_field\": \"has_this_value\"},\ninclude=[\"documents\", \"metadatas\"], # \"ids\" are always included\n)\n</code></pre>"},{"location":"guide/concepts/infra/#adding","title":"Adding","text":"<pre><code>from marvin.infra.chroma import Chroma\nasync with Chroma(collection_name=\"my-new-collection\") as chroma:\nawait chroma.add(\ndocuments=[\"some text\", \"some other text\"],\nmetadatas=[{\"some_metadata_field\": \"some_value\"}, {\"some_metadata_field\": \"some_other_value\"}],\n)\n</code></pre>"},{"location":"guide/concepts/loaders_and_documents/","title":"\ud83c\udfd7\ufe0f Loaders &amp; Documents","text":"<p>Construction zone</p> <p>Loaders are under active development and may change.</p>"},{"location":"guide/concepts/loaders_and_documents/#quickstart","title":"Quickstart","text":"<pre><code>import asyncio\nfrom marvin.loaders.web import SitemapLoader\n# loader that can parse text from all urls in a sitemap\nprefect_docs = SitemapLoader(\nurls=[\"https://docs.prefect.io/sitemap.xml\"],\nexclude=[\"api-ref\"],\n)\n# load, embed, store in Chroma locally at ~/.marvin/chroma/*.parquet\nasyncio.run(prefect_docs.load_and_store())\n</code></pre>"},{"location":"guide/concepts/loaders_and_documents/#_1","title":"Loaders","text":"<p>A <code>Loader</code> parses a source of information into a <code>list[Document]</code>, which can then be stored as context for the LLM.</p> <p></p>"},{"location":"guide/concepts/loaders_and_documents/#what-is-a-document","title":"What is a <code>Document</code>?","text":"<p>A <code>Document</code> is a rich Pydantic model that represents a store-able and searchable unit of information. </p> <p>A valid <code>Document</code> only requires one attribute, <code>text</code>: the raw text of the document.</p> <pre><code>from marvin.models.document import Document\ndocument = Document(text=\"This is a document.\")\n</code></pre> <p>You can attach arbitrary <code>Metadata</code> to a <code>Document</code>.</p> <pre><code>from marvin.models.documents import Document\nfrom marvin.models.metadata import Metadata\nmy_document = Document(\ntext=\"This is a document.\",\nmetadata={\n\"title\": \"My Document\",\n\"link\": \"https://www.example.com\",\n\"random_metadata_field\": \"This is very important to me!\"\n}\n)\n</code></pre>"},{"location":"guide/concepts/loaders_and_documents/#creating-excerpts-from-a-document","title":"Creating excerpts from a <code>Document</code>","text":"<p><code>Document</code> offers a <code>to_excerpts</code> method that splits a <code>Document</code> into a <code>list[Document]</code> which contains rich excerpts of the original.</p> <p><pre><code># using the same document as above\nmy_document.to_excerpts()\n# yields\n[\nDocument(\nid='doc_01GWMH5VA91P0SSYJXE9N7ZK88',\ntext='The following is a original document \\n# Document metadata\\n        Link: https://www.example.com\\nTitle: My Document\\nSource: unknown\\nDocument_type: original\\nRandom_metadata_field: This is very important to me!\\n# Excerpt content: This is a document.',\nembedding=None,\nmetadata=Metadata(\nlink='https://www.example.com',\ntitle='My Document',\nsource='unknown',\ndocument_type='excerpt',\nrandom_metadata_field='This is very important to me!'\n),\nsource=None,\ntype='excerpt',\nparent_document_id='doc_01GWMH58XYXFB8JQYC2T6VMFYC',\ntopic_name='marvin',\ntokens=55,\norder=0,\nkeywords=['document']\n)\n]\n</code></pre> Here, since our <code>Document</code> is short, there's only one excerpt. Longer documents are split into many excerpts according to the <code>chunk_tokens</code> argument of <code>to_excerpts</code>.</p> <p>You'll notice that the <code>Document</code>'s <code>text</code> attribute has been replaced with a rich excerpt that includes the original <code>Document</code>'s <code>Metadata</code> and the excerpt's location in the original <code>Document</code>. This replacement helps provide more context to the LLM when it's searching for answers.</p>"},{"location":"guide/concepts/loaders_and_documents/#how-can-i-create-my-own-loader","title":"How can I create my own <code>Loader</code>?","text":"<p>One way or another, a <code>Loader</code> must return a <code>list[Document]</code>. These <code>Document</code>s can be created in any way you like, but their <code>text</code> must have fewer tokens than the limit for your embedding function. </p> <p>For example, if you're using Marvin's default: OpenAI's <code>text-embedding-ada-002</code>, the limit is 8191 tokens.</p> <p>This is where <code>Document.to_excerpts</code> comes in handy. </p> <p>You can create a <code>Document</code> with a large <code>text</code> attribute, and split it into many <code>Document</code> excerpts to <code>extend</code> the <code>list[Document]</code> you're returning - the bonus being that you'll get rich excerpts as described above.</p>"},{"location":"guide/concepts/loaders_and_documents/#example-pokemonloader","title":"Example: <code>PokemonLoader</code>","text":"<p>For example, one could create a <code>PokemonLoader</code> that loads Pokemon data from the PokeAPI.</p> <pre><code>import httpx\nimport asyncio\nfrom marvin.loaders.base import Loader\nfrom marvin.models.documents import Document\nfrom marvin.models.metadata import Metadata\nasync def fetch_data(url: str) -&gt; dict:\nasync with httpx.AsyncClient() as client:\nreturn (await client.get(url)).json()\nasync def create_document(url: str) -&gt; Document:\npokemon_data = await fetch_data(url)\nspecies_data = await fetch_data(pokemon_data['species']['url'])\nflavor_text = next(\n(\nentry['flavor_text'].replace('\\n', ' ')\nfor entry in species_data['flavor_text_entries']\nif entry['language']['name'] == 'en'\n),\n\"\"\n)\nreturn Document(\ntext=f\"{pokemon_data['name'].capitalize()}: {flavor_text}\",\nmetadata=Metadata(\ntitle=pokemon_data['name'],\npokemon_type=pokemon_data['types'][0]['type']['name']\n)\n)\nclass PokemonLoader(Loader):\n\"\"\"Loads documents from the PokeAPI\"\"\"\nlimit: int = 5\nasync def load(self) -&gt; list[Document]:\nasync with httpx.AsyncClient() as client:\nresponse = await client.get(f\"https://pokeapi.co/api/v2/pokemon?limit={self.limit}\")\ndata = response.json()\ndocuments = await asyncio.gather(\n*[create_document(pokemon['url']) for pokemon in data['results']]\n)\nexcerpts = []\nfor document in documents:\nexcerpts.extend(await document.to_excerpts())\nreturn excerpts\n</code></pre> <p>\u203c\ufe0f Note:</p> <p>Like most of the code in Marvin, the <code>load</code> method must be <code>async</code>.</p> <p>Try it out! - Copy the above code. - Paste it into a Python script or <code>jupyter notebook</code>. - Run <code>await PokemonLoader(limit=5).load()</code>.</p>"},{"location":"guide/concepts/plugins/","title":"\ud83d\udd0c Plugins","text":"<p>Features</p> <p>\ud83e\uddb8 Give bots the ability to access new information and abilities</p> <p>\ud83e\uddbe Turn any function into a bot plugin</p> <p>Plugins extend a bot's functionality by letting it call a function and see the returned value. Plugins must be provided to a bot when it is instantiated, and the bot will decide whether to use a plugin based on its description. Users can influence that choice through instruction (e.g. telling the bot to use a specific plugin). </p>"},{"location":"guide/concepts/plugins/#writing-plugins","title":"Writing plugins","text":"<p>The simplest way to write a plugin is using the <code>@plugin</code> decorator. Note that plugin functions must have a docstring, as this is displayed to the bot so it can decide if it should use a plugin or not.</p> <pre><code>from marvin import Bot, plugin\nimport random\n@plugin\ndef random_number(min:float, max:float) -&gt; float:\n\"\"\"Use this plugin to generate a random number between min and max\"\"\"\nreturn min + (max - min) * random.random()\nbot = Bot(plugins=[random_number])\nawait bot.say('Use the plugin to pick a random number between 41 and 43')\n</code></pre> <p>For more complex plugins, you can inherit from the <code>marvin.Plugin</code> base class and implement a <code>run()</code> method. Class-based plugins must also have a <code>description</code> attribute. This is the equivalent of the function-based plugin above:</p> <pre><code>from marvin import Bot, Plugin\nimport random\nclass RandomNumber(Plugin):\ndescription: str = \"Use this plugin to generate a random number between min and max\"\ndef run(self, min:float, max:float) -&gt; float:\nreturn min + (max - min) * random.random()\nbot = Bot(plugins=[RandomNumber()])\nawait bot.say('Use the plugin to pick a random number between 41 and 43')\n</code></pre>"},{"location":"guide/concepts/plugins/#example-github-issue-search","title":"Example: GitHub Issue Search","text":"<p>To illustrate a real-world use case for plugins, we'll write one that searches for GitHub issues related to a specific topic.</p> <p>Psst!</p> <p>This plugin already exists at <code>marvin.plugins.github.search_github_issues</code>.</p> <p>All we really need to do is write a python function that accepts a <code>query</code> and <code>repo</code> argument, and returns a string summary of the most relevant issues. Remember that whatever arguments the function accepts, the bot will need to be able to provide. </p> <p>So, here's our function signature so far:</p> <pre><code>async def search_github_issues(\nquery: str,\nrepo: str = \"prefecthq/prefect\",\nn: int = 3\n) -&gt; str:\n</code></pre> <p>Luckily, the GitHub API makes the actual implementation easy. We'll use the <code>httpx</code> library to make an API call:</p> <pre><code>headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n# let's increase our rate limit by using an auth token if we have one\nif token := marvin.settings.GITHUB_TOKEN.get_secret_value():\nheaders[\"Authorization\"] = f\"Bearer {token}\"\nasync with httpx.AsyncClient() as client:\nresponse = await client.get(\n\"https://api.github.com/search/issues\",\nheaders=headers,\nparams={\n\"q\": f\"repo:{repo} {query}\",\n\"order\": \"desc\",\n\"per_page\": n,\n},\n)\nresponse.raise_for_status()\n</code></pre> <p>The last thing we need to think about is how we want to present the results. We'll use a Pydantic model to parse our issues and then format them as a string:</p> <pre><code>issues_data = response.json()[\"items\"]\n# enforce 1000 token limit per body\nfor issue in issues_data:\nissue[\"body\"] = slice_tokens(issue[\"body\"], 1000)\nissues = [GitHubIssue(**issue) for issue in issues_data]\nreturn \"\\n\\n\".join(\nf\"{issue.title} ({issue.html_url}):\\n{issue.body}\" for issue in issues\n)\n</code></pre> <p>Note</p> <p>The <code>slice_tokens</code> function is a utility function that limits the number of tokens in a string. It's important to remember that our messages to the LLM must remain under the limit for the model we're using.</p>"},{"location":"guide/concepts/plugins/#all-together-now","title":"All together now","text":"<pre><code>import httpx\nimport marvin\nfrom marvin.loaders.github import GitHubIssue\nfrom marvin.plugins import plugin\nfrom marvin.utilities.strings import slice_tokens\n@plugin\nasync def search_github_issues(\nquery: str, repo: str = \"prefecthq/prefect\", n: int = 3\n) -&gt; str:\n\"\"\"\n    Use the GitHub API to search for issues in a given repository.\n    For example, to search for issues with the label \"bug\" in PrefectHQ/prefect:\n        - repo: prefecthq/prefect\n        - query: label:bug\n    \"\"\"\nheaders = {\"Accept\": \"application/vnd.github.v3+json\"}\nif token := marvin.settings.GITHUB_TOKEN.get_secret_value():\nheaders[\"Authorization\"] = f\"Bearer {token}\"\nasync with httpx.AsyncClient() as client:\nresponse = await client.get(\n\"https://api.github.com/search/issues\",\nheaders=headers,\nparams={\n\"q\": f\"repo:{repo} {query}\",\n\"order\": \"desc\",\n\"per_page\": n,\n},\n)\nresponse.raise_for_status()\nissues_data = response.json()[\"items\"]\nfor issue in issues_data:\nissue[\"body\"] = slice_tokens(issue[\"body\"], 1000)\nissues = [GitHubIssue(**issue) for issue in issues_data]\nreturn \"\\n\\n\".join(\nf\"{issue.title} ({issue.html_url}):\\n{issue.body}\" for issue in issues\n)\n</code></pre>"},{"location":"guide/concepts/plugins/#using-the-plugin","title":"Using the plugin","text":"<p>Now that we have it, we can use our plugin anywhere we have a bot. Let's try it out and search for issues in the <code>marvin</code> repository related to <code>nest_asyncio</code>.</p> <pre><code>from marvin.bot import Bot\nfrom marvin.plugins.github import search_github_issues\nbot = Bot(plugins=[search_github_issues])\nresponse = await bot.say('Any issues about nest_asyncio in PrefectHQ/marvin repo?')\nprint(response.content)\n</code></pre> <p></p>"},{"location":"guide/concepts/plugins/#technical-note-plugin-registration","title":"Technical note: plugin registration","text":"<p>Plugins that inherit from <code>marvin.Plugin</code> automatically register themselves for deserialization based on their class name. Bots are serialized with a reference to the plugin name and load the appropriate plugin upon deserialization. In a situation where you want to avoid conflict, you can manually set the deserialization key:</p> <pre><code>class MyPlugin(marvin.Plugin):\n_discriminator = 'my-key'\n</code></pre> <p>In order for a bot to use a plugin, the plugin must be available and imported prior to the plugin being deserialized. Otherwise it will not be properly registered when the bot is loaded. </p>"},{"location":"guide/concepts/tui/","title":"\ud83d\udda5\ufe0f TUI","text":"<p>Features</p> <p>\ud83d\udda5\ufe0f Full chat interface in your terminal</p> <p>\ud83e\udd16 Create bots with different personalities and objectives</p> <p>\ud83e\uddf5 Create multiple persistent threads</p> <p>Marvin includes a full-featured TUI, or text user interface, that runs entirely in your terminal. The TUI makes it easy to chat with your bots, as well as manage them, for quick non-programmatic interactions.</p>"},{"location":"guide/concepts/tui/#starting-the-tui","title":"Starting the TUI","text":"<p>To start the TUI, type <code>marvin chat</code> into your terminal. You should see this screen: </p> <p>Avoid terminal.app on MacOS</p> <p>On MacOS, the default terminal gives a poor TUI experience. You might prefer another terminal, like iTerm2 or Warp. See the Textual FAQ for more detail.</p>"},{"location":"guide/concepts/tui/#chatting","title":"Chatting","text":"<p>To chat with a bot, begin typing in the input at the bottom of the screen and hit enter. Your message will appear and Marvin will respond. You'll notice your converation appears in the left sidebar as a new thread. </p> <p>Responses are rendered as markdown. Unfortunately, you can't enter multi-line responses yet, but this is a highly-requested Textual feature.</p> <p></p>"},{"location":"guide/concepts/tui/#copying-messages","title":"Copying messages","text":"<p>To copy a message, enter the message editing mode by clicking on it. Then click the \"Copy\" button to copy the message contents to the clipboard. </p>"},{"location":"guide/concepts/tui/#deleting-messages","title":"Deleting messages","text":"<p>You can delete messages if you want to restart your conversation from a certain point. Note that deleting a message also deletes all subsequent messages. To delete a message, enter the message editing mode by clicking on it. Then click the \"Delete\" button. </p>"},{"location":"guide/concepts/tui/#threads","title":"Threads","text":"<p>Each conversation in Marvin is called a \"thread\". Threads are persistent and can have potentially many participants (both bot and human). In the TUI, each thread is a distinct history of messages between you and a bot. </p> <p>Threads are shown in the sidebar on the left side of the screen. You will only see threads that include the bot you're currently talking to.</p>"},{"location":"guide/concepts/tui/#deleting-threads","title":"Deleting threads","text":"<p>To delete a thread, select it from the left hand sidebar and press the \"Delete thread\" button.</p>"},{"location":"guide/concepts/tui/#auto-naming-threads","title":"Auto-naming threads","text":"<p>Marvin does its best to name the threads, taking the message history into account as well as the bot's personality. It will update the thread name until the bot has responded more than five times.</p>"},{"location":"guide/concepts/tui/#bots","title":"Bots","text":"<p>Bots are what set Marvin apart from a simple chat interface. Marvin bots all have personalities and instructions, so you can save bots that are particularly useful to you. You might have one bot that only writes formal emails, another that brainstorms engineering solutions, a third that plays role-playing games, and another that only creates memes. It's completely up to you.</p>"},{"location":"guide/concepts/tui/#changing-bots","title":"Changing bots","text":"<p>To speak to a different bot, press the \"Bots\" button and choose a different bot from the menu. </p>"},{"location":"guide/concepts/tui/#changing-the-default-bot","title":"Changing the default bot","text":"<p>By default, Marvin is selected as the active bot when the TUI starts. To change this, provide a different bot's name when starting the TUI. If the provided name is not found, it will be ignored.</p> <pre><code>marvin chat -b AnotherBot\n</code></pre>"},{"location":"guide/concepts/tui/#creating-and-updating-bots","title":"Creating and updating bots","text":"<p>You can talk to the default Marvin bot to create other bots - just describe the bot you want and, once it collects enough information, it can use plugins to create the bot for you. You can also explore and update existing bots this way. </p> <p>If you prefer a programmatic interface, you can create bots either in Python:</p> <pre><code>from marvin import Bot\nbot = Bot(\nname=\"MyBot\",\ndescription=\"A description\",\npersonality=\"A personality\",\ninstructions=\"Some instructions\",\n)\nbot.save_sync()\n# to update an existing bot with the same name\nbot.save_sync(if_exists='update')\n</code></pre> <p>Or from the CLI:</p> <pre><code>marvin bot create MyBot -d \"A description\" -p \"A personality\" -i \"Some instructions\"\n</code></pre> <p>You can also edit and delete bots the same way.</p>"},{"location":"guide/concepts/tui/#settings","title":"Settings","text":"<p>You can set your OpenAI API key from the TUI by pressing the Settings button. It will be validated and stored in your Marvin config for future sessions, including interactive use outside the TUI.</p> <p></p>"},{"location":"guide/concepts/tui/#upgrading-the-database","title":"Upgrading the database","text":"<p>When new versions of Marvin are released, they may require your database to be upgraded in order to handle the new features or enhancements they contain. Therefore, whenever Marvin starts, it checks to see if your database is up-to-date and prints a helpful warning if it isn't. The TUI can go a step farther and upgrade your database automatically. If it is possible to do so, you will see a screen like the one below.</p> <p></p> <p>Manually upgrading the database</p> <p>We have tried to prevent the TUI from accessing the database until after it runs the upgrade check and shows the warning screen. However, if for some reason it is unable to run the check properly, it may crash. In this case you can manually upgrade the database by running <code>marvin database upgrade</code> in your terminal.</p>"},{"location":"guide/concepts/tui/#technology","title":"Technology","text":"<p>The Marvin TUI is built with Textual, a Python library for building TUIs.</p>"},{"location":"guide/introduction/cli/","title":"CLI","text":"<p>See all available commands:</p> <pre><code>marvin --help\n</code></pre>"},{"location":"guide/introduction/cli/#tui","title":"TUI","text":"<p>To launch the TUI: <pre><code>marvin chat\n</code></pre></p>"},{"location":"guide/introduction/cli/#bot-management","title":"Bot management","text":"<p>To create a new bot: <pre><code>marvin bots create -n MyBot -d \"a description of the bot\" -p \"a personality\"\n</code></pre></p> <p>To list all bots: <pre><code>marvin bots ls\n</code></pre></p> <p>To update a bot: <pre><code>marvin bots update MyBot -p \"a new personality\"\n</code></pre></p> <p>To delete a bot: <pre><code>marvin bots delete MyBot\n</code></pre></p>"},{"location":"guide/introduction/cli/#database-management","title":"Database management","text":"<pre><code>marvin database --help\n</code></pre> <p>WARNING: This will drop all tables and data in the database. <pre><code>marvin database reset\n</code></pre></p>"},{"location":"guide/introduction/cli/#run-the-server","title":"Run the server","text":"<pre><code>marvin server start\n</code></pre>"},{"location":"guide/introduction/cli/#setup-openai","title":"Setup OpenAI","text":"<pre><code>marvin setup-openai\n</code></pre>"},{"location":"guide/introduction/configuration/","title":"Configuration","text":""},{"location":"guide/introduction/configuration/#database","title":"Database","text":"<p>Running Marvin as an API server requires a database. By default, Marvin uses a SQLite database stored at <code>~/.marvin/marvin.sqlite</code>. You can set the database location and type by changing the <code>MARVIN_DATABASE_CONNECTION_URL</code> setting. Marvin is tested with SQLite and Postgres. It may also work with other database supported by SQLAlchemy, so long as there are async drivers available.</p> <p>Warning</p> <p>Marvin's server is under active development, so you should treat its database as ephemeral and able to be destroyed at any time. At this time, Marvin does not include database migrations, which means that upgrading your database schema requires destroying it. This is a high-priority area for improvement.</p>"},{"location":"guide/introduction/configuration/#settings","title":"Settings","text":"<p>Marvin has many configurable settings that can be loaded from <code>marvin.settings</code>.</p>"},{"location":"guide/introduction/configuration/#setting-values","title":"Setting values","text":"<p>All settings can be configured via environment variable using the pattern <code>MARVIN_&lt;name of setting&gt;</code>. For example, to set the log level, set <code>MARVIN_LOG_LEVEL=DEBUG</code> and verify that <code>marvin.settings.log_level == 'DEBUG'</code>. Settings can also be set at runtime through assignment (e.g. <code>marvin.settings.log_level = 'DEBUG'</code>) but this is not recommended because some code might haved loaded configuration on import and consequently never pick up the updated value.</p>"},{"location":"guide/introduction/configuration/#important-settings","title":"Important settings","text":""},{"location":"guide/introduction/configuration/#global","title":"Global","text":"<p>Log level: Set the log level <pre><code>MARVIN_LOG_LEVEL=INFO\n</code></pre> Verbose mode: Logs extra information, especially when the log level is <code>DEBUG</code>.  <pre><code>MARVIN_VERBOSE=true\n</code></pre></p>"},{"location":"guide/introduction/configuration/#openai","title":"OpenAI","text":"<p>API key: Set your OpenAI API key <pre><code>MARVIN_OPENAI_API_KEY=\n</code></pre> Marvin will also respect this global variable <pre><code>OPENAI_API_KEY=\n</code></pre></p> <p>Model name:  Choose the OpenAI model. <pre><code>MARVIN_OPENAI_MODEL_NAME='gpt-4'\n</code></pre></p>"},{"location":"guide/introduction/configuration/#database_1","title":"Database","text":"<p>Database connection URL: Set the database connection URL. Must be a fully-qualified URL. Marvin supports both Postgres and SQLite.</p> <pre><code>MARVIN_DATABASE_CONNECTION_URL=\n</code></pre>"},{"location":"guide/introduction/overview/","title":"The Guide","text":"<p>Welcome to the ever-expanding guide to Marvin.  The documentation is split into a few key sections:</p> <ul> <li>Introduction: information about configuration and basic use</li> <li>Concepts: introductions, explanations, and examples of the most important Marvin concepts</li> <li>Examples: common use cases and examples to help you get started</li> <li>Development: details on developing the Marvin library itself</li> </ul>"},{"location":"guide/use_cases/enforcing_format/","title":"Enforcing AI formatting","text":"<p>One of the most important \"unlocks\" for using AIs alongside and within your code is working with native data structures. This can be challenging for two reasons: first, because LLMs naturally exchange information through unstructured text; and second, because modern LLMs are trained with conversational objectives, so they have a tendency to interject extra words like \"Sure, here's the data you requested:\". This makes extracting structured outputs difficult.</p> <p>Marvin can be used to get AIs to respond in structured, parseable forms. There are two common ways to enable this functionality, depending on whether you're using AI functions or bots. With AI functions, provide a return type annotation. With bots, provide a <code>response_format</code> argument. </p> <p>Under the hood, Marvin is creating a <code>ResponseFormatter</code> object that can handle sending instructions to the AI, validating the response, and parsing the output. It can even take steps to fix invalid responses. Most users will never have to create <code>ResponseFormatters</code> by hand, as Marvin will usually \"do the right thing\" when a return annotation or <code>response_format</code> is provided. </p>"},{"location":"guide/use_cases/enforcing_format/#learn-more","title":"Learn more","text":"<p>For more detail, see the bots docs.</p>"},{"location":"guide/use_cases/enforcing_format/#examples","title":"Examples","text":"<p>Examples are shown for both AI functions and bots.</p>"},{"location":"guide/use_cases/enforcing_format/#returning-a-string","title":"Returning a string","text":"<p><pre><code>@ai_fn\ndef my_fn() -&gt; str:\n\"\"\"This function will return a string\"\"\"\n</code></pre> <pre><code>Bot() # bots return strings by default\n</code></pre></p>"},{"location":"guide/use_cases/enforcing_format/#returning-a-list-of-dicts","title":"Returning a list of dicts","text":"<p><pre><code>@ai_fn\ndef my_fn() -&gt; list[dict]:\npass\n</code></pre> <pre><code>Bot(response_format=list[dict])\n</code></pre></p>"},{"location":"guide/use_cases/enforcing_format/#pydantic-models","title":"Pydantic models","text":"<p><pre><code>class MyOutput(pydantic.BaseModel):\nx: int\ny: list[dict]\n@ai_fn\ndef my_fn() -&gt; list[MyOutput]:\n\"\"\"This function will return a list of MyOutput models\"\"\"\n</code></pre> <pre><code>Bot(response_format=MyOutput)\n</code></pre></p>"},{"location":"guide/use_cases/enforcing_format/#json-objects","title":"JSON objects","text":"<p>Instead of using Python types, you can describe the shape of the output. If your description includes the word \"json\", it will be automatically parsed and validated; otherwise it will be returned as a string</p> <p>For example, these will both return structured objects: <pre><code>@ai_fn\ndef my_fn() -&gt; 'a JSON list of strings and ints':\n\"\"\"This function will return list[str | int]\"\"\"\n</code></pre> <pre><code>Bot(response_format = 'a JSON list of strings and ints')\n</code></pre></p> <p>While these will return strings (that could be parsed with <code>json.loads()</code>). Note the absence of the word JSON, which is what hints to Marvin to add a JSON parser to the <code>ResponseFormatter</code>.</p> <p><pre><code>@ai_fn\ndef my_fn() -&gt; 'a list of strings and ints':\n\"\"\"This function will return list[str | int]\"\"\"\n</code></pre> <pre><code>Bot(response_format = 'a list of strings and ints')\n</code></pre></p>"}]}